{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af681996-7829-47a7-ada4-f15e0f91d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session successfully started.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UnemploymentPrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session successfully started.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f93fae2-fd1d-4b6e-abf4-83d834dfc2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 312596\n",
      "root\n",
      " |-- Year: long (nullable = true)\n",
      " |-- State_FIPS: string (nullable = true)\n",
      " |-- Total_Labor_Force: long (nullable = true)\n",
      " |-- Total_Employed: long (nullable = true)\n",
      " |-- Total_Unemployed: long (nullable = true)\n",
      " |-- State_Unemployment_Rate: double (nullable = true)\n",
      " |-- OCC_CODE: string (nullable = true)\n",
      " |-- OCC_TITLE: string (nullable = true)\n",
      " |-- TOT_EMP: double (nullable = true)\n",
      " |-- A_MEAN: double (nullable = true)\n",
      " |-- A_MEDIAN: double (nullable = true)\n",
      " |-- A_PCT10: double (nullable = true)\n",
      " |-- A_PCT90: double (nullable = true)\n",
      "\n",
      "+----+----------+-----------------+--------------+----------------+-----------------------+--------+-----------------------------------+-------+--------+--------+--------+-------+\n",
      "|Year|State_FIPS|Total_Labor_Force|Total_Employed|Total_Unemployed|State_Unemployment_Rate|OCC_CODE|OCC_TITLE                          |TOT_EMP|A_MEAN  |A_MEDIAN|A_PCT10 |A_PCT90|\n",
      "+----+----------+-----------------+--------------+----------------+-----------------------+--------+-----------------------------------+-------+--------+--------+--------+-------+\n",
      "|2015|01        |2172619          |2040086       |132533          |6.10014917479779       |11-0000 |Management Occupations             |69100.0|109800.0|95470.0 |52320.0 |NULL   |\n",
      "|2015|01        |2172619          |2040086       |132533          |6.10014917479779       |11-1011 |Chief Executives                   |1100.0 |210530.0|NULL    |109410.0|NULL   |\n",
      "|2015|01        |2172619          |2040086       |132533          |6.10014917479779       |11-1021 |General and Operations Managers    |27240.0|122410.0|102330.0|57800.0 |NULL   |\n",
      "|2015|01        |2172619          |2040086       |132533          |6.10014917479779       |11-1031 |Legislators                        |1430.0 |22690.0 |18380.0 |16110.0 |32120.0|\n",
      "|2015|01        |2172619          |2040086       |132533          |6.10014917479779       |11-2011 |Advertising and Promotions Managers|50.0   |111590.0|85890.0 |51580.0 |NULL   |\n",
      "+----+----------+-----------------+--------------+----------------+-----------------------+--------+-----------------------------------+-------+--------+--------+--------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "parquet_path = 'data/integrated_bls_oews_state_year_occ.parquet'\n",
    "df_spark = spark.read.parquet(parquet_path)\n",
    "\n",
    "print(f\"Total records loaded: {df_spark.count()}\")\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5, truncate=False)\n",
    "\n",
    "df_spark = df_spark.withColumn(\"label\", df_spark[\"State_Unemployment_Rate\"].cast(\"double\"))\n",
    "df_spark = df_spark.drop(\"State_Unemployment_Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071304e1-19b0-4473-b4b0-e0d2ad458d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values in TOT_EMP with median: 830.00\n",
      "Filled missing values in A_MEAN with median: 50420.00\n",
      "Filled missing values in A_MEDIAN with median: 47600.00\n",
      "Filled missing values in A_PCT10 with median: 31750.00\n",
      "Filled missing values in A_PCT90 with median: 71970.00\n",
      "\n",
      "Training records: 249954, Test records: 62642\n"
     ]
    }
   ],
   "source": [
    "#fill na vals with median\n",
    "cols_to_fill = [\"TOT_EMP\", \"A_MEAN\", \"A_MEDIAN\", \"A_PCT10\", \"A_PCT90\"] \n",
    "\n",
    "for col in cols_to_fill:\n",
    "    try:\n",
    "        median_value = df_spark.approxQuantile(col, [0.5], 0.01)[0]\n",
    "        df_spark = df_spark.fillna(median_value, subset=[col])\n",
    "        print(f\"Filled missing values in {col} with median: {median_value:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating median for {col}: {e}. Skipping or check data quality.\")\n",
    "\n",
    "\n",
    "\n",
    "categorical_cols = [\"State_FIPS\", \"OCC_CODE\"]\n",
    "indexed_cols = [col + \"_Index\" for col in categorical_cols]\n",
    "encoded_cols = [col + \"_Vec\" for col in categorical_cols]\n",
    "\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"keep\") \n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# OneHotEncoder\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "feature_cols = [\n",
    "    \"Year\", \"Total_Labor_Force\", \"Total_Employed\", \"Total_Unemployed\",\n",
    "    \"TOT_EMP\", \"A_MEAN\", \"A_MEDIAN\", \"A_PCT10\", \"A_PCT90\"\n",
    "] + encoded_cols\n",
    "\n",
    "for col in feature_cols:\n",
    "    if col not in encoded_cols:\n",
    "        df_spark = df_spark.withColumn(col, df_spark[col].cast(\"double\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# split test_trainiing data\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"\\nTraining records: {train_data.count()}, Test records: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2db966d-6dc9-4e79-82bd-cf983c6b3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training (GBT Regressor) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "+----------------+-----------------+\n",
      "|           label|       prediction|\n",
      "+----------------+-----------------+\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "|6.10014917479779|5.157711044808522|\n",
      "+----------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\", \n",
    "    maxIter=15,         \n",
    "    maxDepth=5,          \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
    "\n",
    "print(\"\\n--- Starting Model Training (GBT Regressor) ---\")\n",
    "gbt_model = pipeline.fit(train_data) \n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "predictions = gbt_model.transform(test_data)\n",
    "predictions.select(\"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f72b6b0-be2e-4536-afcc-83de3659c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation Results ---\n",
      "R-squared (R2): 0.9069\n",
      "Mean Absolute Error (MAE): 0.3979\n",
      "\n",
      "⚠️ Model still does NOT meet project targets (R2 >= 0.85, MAE <= 0.005).\n",
      "Next step: Hyperparameter tuning (CrossValidator) and/or adding ACS PUMS data.\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "#  R-squared\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"\\n--- Model Evaluation Results ---\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "if r2 >= 0.85 and mae <= 0.005: \n",
    "    print(\"\\n✅ Model meets project targets!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Model still does NOT meet project targets (R2 >= 0.85, MAE <= 0.005).\")\n",
    "    print(\"Next step: Hyperparameter tuning (CrossValidator) and/or adding ACS PUMS data.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97c8a1-c4c7-462e-831b-f950f48604de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b6c6b-73e6-4b48-ac9c-6e6b7250b5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef5e5f-3e86-478e-bc71-d3caa720f1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
