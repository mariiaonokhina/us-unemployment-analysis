{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aaf7a9-ab78-4cb9-abb5-84e87245db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path to import pums_loader\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from pums_loader import (\n",
    "    process_pums_pipeline,\n",
    "    load_all_years,\n",
    "    write_parquet,\n",
    "    validate_folder_structure,\n",
    "    print_validation_report,\n",
    "    REQUIRED_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265eb689-b461-4b60-9511-615c025e8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Define absolute paths (notebooks run inside /notebooks directory)\n",
    "# Project root is one level up from notebooks/\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, 'data', 'raw', 'pums')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'data', 'processed', 'parquet_pums')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7f9db-cacc-44b7-be73-f98eb4f0b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE SPARK SESSION\n",
    "# ============================================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PUMS Data Pipeline\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark session created\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ab2e0-4d22-4c2a-b807-cb763bfbcd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 10:51:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: VALIDATE FOLDER STRUCTURE\n",
    "# ============================================================================\n",
    "# This step is optional but helpful to see what data is available\n",
    "\n",
    "validation = validate_folder_structure(RAW_DATA_DIR)\n",
    "print_validation_report(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ca644-e512-444f-a81f-045dc58dcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD 1: USE THE COMPLETE PIPELINE (RECOMMENDED)\n",
    "# ============================================================================\n",
    "# This single function call does everything:\n",
    "# - Validates folder structure\n",
    "# - Loads all years\n",
    "# - Combines them\n",
    "# - Writes to parquet\n",
    "\n",
    "df_all = process_pums_pipeline(\n",
    "    spark=spark,\n",
    "    raw_data_dir=RAW_DATA_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    columns=REQUIRED_COLUMNS,  # Use default columns, or pass custom list\n",
    "    skip_year=2020,  # Skip 2020 (experimental data)\n",
    "    write_individual_years=True,  # Write both combined and individual files\n",
    "    validate_first=True  # Validate folder structure first\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88634d-7147-4648-aab1-dc89c2b6fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No year data found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# METHOD 2: STEP-BY-STEP APPROACH (FOR MORE CONTROL)\n",
    "# ============================================================================\n",
    "# Uncomment the code below if you want more control over each step\n",
    "\n",
    "# # Step 1: Load all years\n",
    "# df_all = load_all_years(\n",
    "#     spark=spark,\n",
    "#     raw_data_dir=RAW_DATA_DIR,\n",
    "#     columns=REQUIRED_COLUMNS,\n",
    "#     skip_year=2020\n",
    "# )\n",
    "\n",
    "# # Step 2: Write to parquet\n",
    "# if df_all is not None:\n",
    "#     combined_path, individual_paths = write_parquet(\n",
    "#         df_all,\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#         mode=\"overwrite\",\n",
    "#         write_individual_years=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7720ce4-fb8e-40fd-98d3-c5ac8909eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPLORE THE LOADED DATA\n",
    "# ============================================================================\n",
    "\n",
    "if df_all is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nSchema:\")\n",
    "    df_all.printSchema()\n",
    "    \n",
    "    # Show row count\n",
    "    total_rows = df_all.count()\n",
    "    print(f\"\\nTotal rows: {total_rows:,}\")\n",
    "    \n",
    "    # Show column count\n",
    "    print(f\"Total columns: {len(df_all.columns)}\")\n",
    "    \n",
    "    # Show years available\n",
    "    if \"YEAR\" in df_all.columns:\n",
    "        years = df_all.select(\"YEAR\").distinct().orderBy(\"YEAR\").collect()\n",
    "        year_list = [row[\"YEAR\"] for row in years]\n",
    "        print(f\"Years: {year_list}\")\n",
    "        \n",
    "        # Show row counts by year\n",
    "        print(\"\\nRows by year:\")\n",
    "        year_counts = df_all.groupBy(\"YEAR\").count().orderBy(\"YEAR\").collect()\n",
    "        for row in year_counts:\n",
    "            print(f\"  {row['YEAR']}: {row['count']:,} rows\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nSample data (first 5 rows):\")\n",
    "    df_all.show(5, truncate=False)\n",
    "    \n",
    "    # Show some basic statistics\n",
    "    print(\"\\nBasic statistics for numeric columns:\")\n",
    "    numeric_cols = [\"AGEP\", \"WAGP\", \"PINCP\"]\n",
    "    available_numeric = [c for c in numeric_cols if c in df_all.columns]\n",
    "    if available_numeric:\n",
    "        df_all.select(available_numeric).describe().show()\n",
    "else:\n",
    "    print(\"❌ No data loaded. Check the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# READ BACK THE PARQUET FILES (EXAMPLE)\n",
    "# ============================================================================\n",
    "# You can read the parquet files back like this:\n",
    "\n",
    "# Read combined file\n",
    "# df_combined = spark.read.parquet(os.path.join(OUTPUT_DIR, \"pums_all.parquet\"))\n",
    "\n",
    "# Read a specific year\n",
    "# df_2019 = spark.read.parquet(os.path.join(OUTPUT_DIR, \"pums_2019.parquet\"))\n",
    "\n",
    "# Verify the data\n",
    "# print(f\"Combined rows: {df_combined.count():,}\")\n",
    "# print(f\"2019 rows: {df_2019.count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d588d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP (OPTIONAL)\n",
    "# ============================================================================\n",
    "# Uncomment to stop Spark session when done\n",
    "\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
